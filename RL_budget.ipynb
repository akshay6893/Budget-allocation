{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 620,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque \n",
    "import numpy as np\n",
    "import random\n",
    "import gym\n",
    "import time as lp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The input variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 600,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = 4\n",
    "actions = 2           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 601,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 602,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 603,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.04395843, -0.03976584,  0.04555846,  0.0064775 ])"
      ]
     },
     "execution_count": 604,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 605,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense,Input\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 614,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_size = 7 \n",
    "action_size = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 624,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QAgent():\n",
    "    \n",
    "    def __init__(self,state_size,action_size):        \n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size        \n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.gamma = 0.9\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.epsilon_min = 0.01\n",
    "        self.learning_rate = 0.001\n",
    "        \n",
    "        self.model = self._build_model()\n",
    "    \n",
    "    def _build_model(self):\n",
    "        \n",
    "        \n",
    "        model = Sequential()\n",
    "        model.add(Dense(24,input_dim = self.state_size,activation=\"relu\"))          \n",
    "        model.add(Dense(24,activation = \"relu\"))\n",
    "        model.add(Dense(self.action_size,activation = \"relu\"))\n",
    "        \n",
    "        model.compile(loss=\"mse\",optimizer='Adam')  \n",
    "                  \n",
    "        return model\n",
    "    \n",
    "    def remember(self,state,action,reward,next_state,done):\n",
    "        self.memory.append((state,action,reward,next_state,done))\n",
    "    \n",
    "    def act(self,state,z,a1) :\n",
    "        min_numb = 50           \n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            if all(p<min_numb for p in a1):\n",
    "                return np.argmax([z/v for v in a1])\n",
    "            else:\n",
    "                return random.randrange(self.action_size)\n",
    "        act_values = self.model.predict(state)\n",
    "        return np.argmax(act_values[0])\n",
    "    \n",
    "    def replay (self, batch_size):       \n",
    "        minibatch = random.sample(self.memory,batch_size)       \n",
    "        for state,action,reward,next_state,done in minibatch : \n",
    "            target = reward\n",
    "            if not done:\n",
    "                target = (reward + self.gamma*np.amax(self.model.predict(next_state)[0]))\n",
    "            target_f = self.model.predict(state)\n",
    "            target_f[0][action] = target                \n",
    "            self.model.fit(state,target_f,epochs = 1, verbose = 0)        \n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "                \n",
    "    def load(self,name):\n",
    "        self.model.load_weights(name)\n",
    "    \n",
    "    def save(self,name):\n",
    "        self.model.save_weights(name)\n",
    "    \n",
    "\n",
    "def initializer():\n",
    "    with request.urlopen('https://app.nextel.io/parse/getFbStorePost/920') as response:\n",
    "            source = response.read()\n",
    "            data = json.loads(source)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    data1 = pd.DataFrame()\n",
    "    for c in range(len(data)):\n",
    "        tu = pd.DataFrame(data[c],index=[c])\n",
    "        data1 = data1.append(tu)\n",
    "\n",
    "    data1['impression'] = data1[\"impression\"].astype(int)\n",
    "    data1.reset_index(drop=True)\n",
    "    \n",
    "    return data1\n",
    "\n",
    "def reset(campaign_id):\n",
    "    \n",
    "    PPR = 1000 \n",
    "    \n",
    "    data1 = initializer()\n",
    "\n",
    "    ind_ = list(data1[data1['campaign_id']== f'{campaign_id}'].index)[0]\n",
    "\n",
    "    vnm = data1['adsetsids'][ind_]      \n",
    "    adset1 = vnm.split('\"')\n",
    "    adset=[]\n",
    "    for oo in range(len(adset1)):\n",
    "        if oo%2!=0:\n",
    "            adset.append(adset1[oo])\n",
    "    adset\n",
    "    spend = 0\n",
    "    budget_dict = {}   \n",
    "    reset_dict = {}   \n",
    "    for q in json.loads(data1['adset_breakdown'][ind_])['age'][adset[0]]['data']:\n",
    "        spend += float(q['spend'])\n",
    "        chk = []\n",
    "        try:\n",
    "            for r in q['cost_per_action_type']:\n",
    "                chk.append(r['action_type'])\n",
    "                if r['action_type']== 'post_reaction':  \n",
    "                    reset_dict[q['age']] = (PPR/(float(r['value'])))*(float(q['spend'])/float(r['value']))\n",
    "                    budget_dict[q['age']] = float(q['spend'])\n",
    "            if 'post_reaction' not in chk:\n",
    "                reset_dict[q['age']] = 0\n",
    "                budget_dict[q['age']] = float(q['spend'])\n",
    "        except Exception:\n",
    "            reset_dict[q['age']] = 0\n",
    "            budget_dict[q['age']] = float(q['spend'])\n",
    "\n",
    "\n",
    "    total = sum(reset_dict.values())\n",
    "    reset_dict1 = {k:(l/total)*spend for k,l in reset_dict.items()}   \n",
    "\n",
    "    state_after_reset = np.array(list(reset_dict1.values()))\n",
    "    \n",
    "    return state_after_reset,reset_dict\n",
    "\n",
    "\n",
    "def single_action_to_full(reset_dict,action):\n",
    "\n",
    "    total = sum(reset_dict.values())\n",
    "    reset_dict_pct = {k:(l/total)*100 for k,l in reset_dict.items()}\n",
    "\n",
    "    \n",
    "\n",
    "    range_of_profit_m = [(0,15),(15,25),(25,35),(35,45),(45,55),(55,100)]\n",
    "    pct_alloted = {'0':1,'15':3,'25':5,'35':6,'45':8,'55':10}\n",
    "\n",
    "    x = list(reset_dict_pct.values())[action]\n",
    "    _pct = list(reset_dict_pct.values())[action]      \n",
    "    for h in range_of_profit_m:\n",
    "        if int(x) in range(h[0],h[1]):\n",
    "            increase_pct = (pct_alloted[str(h[0])])\n",
    "            increased_pct = (increase_pct+ _pct)\n",
    "            key_abe = (list(reset_dict_pct.keys())[action])        \n",
    "\n",
    "    reset_dict_del = dict(reset_dict_pct)\n",
    "    reset_dict_del.pop(key_abe)\n",
    "\n",
    "    tot = sum(list(reset_dict_del.values()))\n",
    "    reset_dict_pct_left = {k: (((l/tot)*increase_pct)) for k,l in reset_dict_del.items() }\n",
    "\n",
    "\n",
    "    key_link = []\n",
    "    no_zero_dict = {}\n",
    "    for k,y in reset_dict_pct_left.items():\n",
    "        if y!=0:\n",
    "            no_zero_dict[k] = y \n",
    "        else:\n",
    "            key_link.append(k)\n",
    "\n",
    "        without_zeros = len(no_zero_dict)\n",
    "\n",
    "    new_pct_dist = {}\n",
    "\n",
    "    i = 0\n",
    "    o = 0\n",
    "\n",
    "    for b in range(without_zeros):\n",
    "        next_ = max(reset_dict_pct_left.values())\n",
    "        left = increase_pct-next_\n",
    "        left_key = max(reset_dict_pct_left,key=reset_dict_pct_left.get)\n",
    "        if reset_dict_pct[left_key]*0.4>left:\n",
    "            new_pct_dist[left_key] = left\n",
    "        else:\n",
    "            new_left = left-reset_dict_pct[left_key]*0.4\n",
    "            new_pct_dist[left_key] = reset_dict_pct[left_key]*0.4\n",
    "            if i==without_zeros:\n",
    "                o+=new_left\n",
    "            else:\n",
    "                next_ += new_left\n",
    "        reset_dict_pct_left.pop(left_key)\n",
    "        rest_pct = next_\n",
    "        i += 1\n",
    "    if o!=0:\n",
    "        increased_pct += o\n",
    "\n",
    "\n",
    "    reset_dict_pct[key_abe] = increased_pct\n",
    "\n",
    "    new_pct_dist[key_abe]=0\n",
    "    for b in key_link:\n",
    "        new_pct_dist[b]=0\n",
    "\n",
    "    final = {k: ((reset_dict_pct[k]-new_pct_dist[k])/100)*spend for k,i in reset_dict_pct.items()}\n",
    "    next_state = np.array(list(final.values()))\n",
    "    \n",
    "    return next_state\n",
    "\n",
    "def Step(action,campaign_id):\n",
    "    one ,two = reset(campaign_id)\n",
    "    old_sum_PM = sum(list(two.values()))\n",
    "    next_state = single_action_to_full(two,action)   \n",
    "    lp.sleep(10)                                            \n",
    "    one ,two_new = reset(campaign_id)\n",
    "    new_sum_PM = sum(list(two_new.values()))\n",
    "    reward = (new_sum_PM-old_sum_PM)*100\n",
    "    if (abs(new_sum_PM-old_sum_PM)/new_sum_PM)*100 > 25:\n",
    "        done = True\n",
    "    else:\n",
    "        done = False\n",
    "    return next_state,reward,done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 625,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = QAgent(state_size = state_size, action_size = action_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "done = False \n",
    "z = 0\n",
    "a1 = [1,1,1,1,1,1,1]       \n",
    "for e in range(episodes):  \n",
    "    \n",
    "    campaign_id = 6010678584024\n",
    "    state,_ = reset(campaign_id)\n",
    "    state =  state.reshape((1,state_size))  \n",
    "                                            \n",
    "                                            \n",
    "                                   \n",
    "    \n",
    "    for time2 in range(2):   \n",
    "        \n",
    "        action = agent.act(state,z,a1)\n",
    "        \n",
    "        z+=1\n",
    "        \n",
    "        a1[action]+=1\n",
    "        \n",
    "        next_state, reward, done = Step(action,campaign_id)\n",
    "        \n",
    "        reward = reward if not done else -10  \n",
    "        \n",
    "        next_state = next_state.reshape((1,state_size))\n",
    "        \n",
    "        agent.remember(state,action,reward,next_state,done) \n",
    "        \n",
    "        state = next_state\n",
    "        \n",
    "#         if done :\n",
    "        print(f\"episodes : {e}/{episodes} , score : {time2} , epsilon : {agent.epsilon:.2} \")\n",
    "#         break\n",
    "\n",
    "    if len(agent.memory) > batch_size:      # Save model \n",
    "        agent.replay(batch_size)\n",
    "        \n",
    "    \n",
    "    agent.save(directory+'weights'+'{:04d}'.format(e)+\".hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
